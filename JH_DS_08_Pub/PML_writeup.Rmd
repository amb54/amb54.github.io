---
title: "PML-Writeup"
output: html_document
---

###Introduction  
The goal of this project is to find a model that takes data from accelerometers placed on belt, forearm, arm and dumbbell to predict how well the excercise barbell lifts are done.  
Data was collected from 6 individuals performing the excercise in 5 different ways in the range from correctly to incorrectly. While doing this they had accelerometers placed on belt, forearm,arm and dumbbell.  
Reference:  
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.


```{r,echo=FALSE}
##---Libraries---
library(grid)
library(splines)
library(survival)
library(lattice)
library(ggplot2)
library(caret)
library(randomForest)

```
(Sorry loading the packages, needed for this project, into R creates this and I do not know how to avoid it.)


###Data: loading, cleaning and variable analysis
```{r}
##---ImportData into R---
pmlTrain0<-read.csv("pml-training.csv",stringsAsFactors=FALSE)
pmlTest0<-read.csv("pml-testing.csv",stringsAsFactors=FALSE)
```

Two data sets were imported. One training set and one testing set. The dimensions of the training set was
19622 observations and 160 variables. The dimensions for the testing set was just 20 observations and also with 160 variables. (Figure 1 in the Appendix).  
Initially the following examinations were done on the two data sets:  
1) Check if the variable names the same in the "pml-training"data set and the "pml-testing" data set.    
2) Check if there are any NAs, empty columns, irrelevant data etc.   
3) Find what variables are related to the accelerometers: belt, forearm, arm, dumbbell.  
4) Check if there are any difference in the classes (character, numerical etc.) for the variables in the "pml-training"data set and the "pml-testing" data set.  

#####1) Variable names  
The two data sets had the same names for all variables except for the last one (column 160). In the case of the training set this column has the name "classe" which is the outcome. In the testing data set this last column just contains a problem_id.
```{r}
##Check if names of objects(variables) are the same in training and test set
namePmlTrain0<-names(pmlTrain0);namePmlTest0<-names(pmlTest0)
which(namePmlTrain0!=namePmlTest0)
```


#####2) Empty columns  
First the testing data set was examined. With only 20 observations and 160 variables this was very doable.
Many column were totaly empty or was fully set to NA. The names of these columns indicated that they were related to statistical calculations of the collected data.  
A look at the training data set showd that also in this case there were a lot of emty/NA columns.  
A comparison was made to see if both data sets had the same columns empty, and the result gave that this acctually was the case.  

```{r}
firstRowTest<-as.character(pmlTest0[1,])
replaceStep1<-firstRowTest==""; firstRowTest[replaceStep1]<-"NA"
colWithData1<-which(firstRowTest!="NA")

firstRowTrain<-as.character(pmlTrain0[1,])
replaceStep2<-firstRowTrain==""; firstRowTrain[replaceStep2]<-"NA"
colWithData2<-which(firstRowTrain!="NA")

which(colWithData1!=colWithData2)
```


However, the training set had some rows with data in these columns. Examining this gave that 406 rows out of 19622 had data in the otherwise empty columns. That corresponds to 2.1%, which should not affect the outcome when training a model.

```{r}
check1<-which(pmlTrain0$kurtosis_roll_belt!="")
length(check1); length(check1)/dim(pmlTrain0)[1]
```




The empty/NA columns were removed, and the following columns were kept in both data sets.  

```{r}
pmlTrain1<-pmlTrain0[,colWithData1]; pmlTest1<-pmlTest0[,colWithData1]
colWithData1
```



#####3) Variables related to: belt, forearm, arm, dumbbell  

The following variable names in the testing data set were not related to either belt, forearm, arm or dumbbell.

```{r}
relatedVariables<-c(grep(".*belt.*|.*arm.*|.*dumbbell.*",namePmlTest0))
##What variables that are NOT related to belt,forearm,arm and dumbell
notRelated<-namePmlTest0[-relatedVariables]
notRelated
```
A further investigation of these variables showed that 3 of them were related to timestamps, 2 seemed to be related to the statistical calculations, 2 were id numbers for the observations and 1 was the username.  
All these vareables except for user_name and problem_id(classe in the case of the training data set) were omitted in both data sets.  
The resulting data sets had the following dimensions:  
Training data set: 19622 observations, 53 predictors (the column 54 was the outcome, classe)  
Testing data set: 20 observations, 53 predictors  (the column 54 was the problem_id)

```{r}
col<-c(1,3:7)
pmlTrain2<-pmlTrain1[,-col]; pmlTest2<-pmlTest1[,-col]
dim(pmlTrain2);dim(pmlTest2)
```


#####4) Classes of the variables

In this section it was made sure that the classes of the variables in both data sets were identical
```{r}
##Check if class of objects(variables) are the same
classPmlTrain2<-sapply(pmlTrain2[,-54],class); classPmlTest2<-sapply(pmlTest2[,-54],class)
classDiff<-which(classPmlTrain2!=classPmlTest2)
classDiff               

##Find class in the test-set, of the variables that differ
classInTest<-sapply(pmlTest2[,c(40,52,53)],class)
classInTest

##   Change class in pmlTrain2 for columns 40,52,53
pmlTrain2[,40]<-as.integer(pmlTrain2[,40])
pmlTrain2[,52]<-as.integer(pmlTrain2[,52])
pmlTrain2[,53]<-as.integer(pmlTrain2[,53])

```


```{r}
##Change variables(user_name, classe) to class:factor
pmlTrain2$user_name<-factor(pmlTrain2$user_name)
pmlTest2$user_name<-factor(pmlTest2$user_name)
pmlTrain2$classe<-factor(pmlTrain2$classe)
```


###Data split

To be able to estimate the the out of sample error, the training data set was split accoding to  
training 60%  
cross validation 20%  
testing 20%   
Figure 2 in the appendix shows the dimensions of these three datasets

```{r}
##---Data split---
set.seed(33833)
inTrain<-createDataPartition(y=pmlTrain2$classe,p=0.6,list=FALSE)
training<-pmlTrain2[inTrain,]
cvTest<-pmlTrain2[-inTrain,]

set.seed(33833)
inTest<-createDataPartition(y=cvTest$classe,p=0.5,list=FALSE)
crossValidation<-cvTest[inTest,]
testing<-cvTest[-inTest,]
```







### Train with randomForest()

#####Using 53 predictors

The machine learning algorithm used was Breiman and Cutler's random forest.  
http://cran.r-project.org/web/packages/randomForest/index.html  
The estimated OOB (out of bag) error rate is really small, less than 1%.

```{r buildModelRandomForest,cache=TRUE}
##Train with randomForest()
set.seed(33833)
modelFitRF<-randomForest(classe~.,data=training,importance=TRUE)
modelFitRF
```


According to Breiman and Cutler quote: "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run"  
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr   
Anyhow, here is a prediction using the model and the crossValidation data set.  
Calculation of the estimated of error rate is done via the confusion matrix (Figure 3 in the Appendix) and was also estimated to be less than 1%.  

```{r}
##Predict with CrossValidation dataSet
predRandomForestCV<-predict(modelFitRF,newdata=crossValidation)
conMat<-confusionMatrix(predRandomForestCV,crossValidation$classe)
```

```{r}
##Estimating error rate via the confusion matrix
##Sorry this is done by hand and might vary very slightly in another run.
(4+1+4+1+7+3+1)/(1115+745+680+635+718+4+1+4+1+7+3+1)
```

#####Using 7 predictors

In the above discussion the number of variables was redused from 160 to 53.The model fit used these 53 variable. It might still be possible to reduce the number of variables by looking at the importance of the variables. The argument importance in randomForest() evaluates the importance of the predictors. Figure 4 in the Appendix shows a graph of the mean decrease in Gini index for the thirty most important predictors.   
Looking at the graph, 7 predictors stands out as more important than the others.  
They can be found in the coulmns 2 4 40 42 3 39 41 in the training, crossValidation, testing data sets as shown below

```{r}
impVariables<-order(importance(modelFitRF,type=2),decreasing=TRUE)
impVar7<-impVariables[1:7];impVar7
```
These columns corresponds to the following variables:  
```{r}
names(training[,impVar7])
```

Building a model just using these 7 variables results in a OOB estimation of under 2%.

```{r}
set.seed(33833)
modelFitRF7<-randomForest(classe~.,data=training[,c(impVar7,54)],importance=TRUE)
modelFitRF7
```


Here is a prediction using the 3rd part of the split data set, the testing data set. The confusion Matrix is shown in Figure 5 in the Appendix. Estimating the error rate via the confusion matrix results in an estimation of less than 2%.

```{r}
##Predict with testing dataSet
predRandomForestCV7<-predict(modelFitRF7,newdata=testing[,c(impVar7,54)])
conMat7<-confusionMatrix(predRandomForestCV7,testing[,c(impVar7,54)]$classe)
```

```{r}
##Estimating error rate via the confusion matrix
##Sorry this is done by hand and might vary very slightly in another run.
(3+9+11+1+5+6+8+3+2+1+2+4+3+1)/(1100+746+669+638+711+3+9+11+1+5+6+8+3+2+1+2+4+3+1)
```


### Prediction of the 20 cases in the data set pml-testing

To do this, the model build with only 7 predictors (OOB error rate <2%, and accuracy for the testing set 98.5% ) was used

```{r}
prediction<-predict(modelFitRF7,newdata=pmlTest2)
prediction
```

### Conlusion  
The model trained by randomForest worked well when predicting the 20 cases presented in the pml-testing data set. All twenty were submitted, and proved to be correct.  
The accelerometers placed on the belt, forearm and dumbbell were included in the list of the seven most important predictors for predicting the outcome of the performance of how well the exercise was done. The accelerometer placed on the arm did not seem to contribute to the prediction as much.


##Appendix  

####Figure 1  
Dimensions of the raw files pml-training and pml-testing

```{r}
dim(pmlTrain0); dim(pmlTest0)
```

####Figure 2  
Dimensions of the split data sets: training, crossValidation, testing

```{r}
dim(training);dim(crossValidation);dim(testing)
```

####Figure 3  
Confusion matrix for the prediction using the model built with 53 predictors, modelFitRF and the crossValidation data set.

```{r}
conMat
```


####Figure 4  
Graph of the 30 most important variables in the model modelFitRF

```{r}
varImpPlot(modelFitRF,type=2,sort=TRUE,main="",cex=0.8)
```


####Figure 5    
Confusion matrix for the prediction using the model built with 7 predictors, modelFitRF7, and the testing data set.

```{r}
conMat7
```
